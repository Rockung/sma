{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification\n",
    "We are now going to develop a more powerful approach to image classification that we will eventually naturally extend to entire Neural Networks and Convolutional Neural Networks. The approach will have two major components:\n",
    "\n",
    "* a **score function**: mapping the raw data to class scores\n",
    "* a **loss function**: quantifying the agreement between the predicted scores and the ground truth labels\n",
    "\n",
    "We will then cast this as an optimization problem in which we will minimize the loss function with respect to the parameters of the score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier\n",
    "\n",
    "Each image \\\\(x_i\\\\) is associated with a label \\\\(y_i\\\\) in training dataset of N samples and K classes. The image \\\\(x_i\\\\) can be flatten out to a single column vector of shape[Dx1]. So we build a linear score function \\\\(f: R^D \\mapsto R^K\\\\) to get the scores of every label for the image.\n",
    "\n",
    "$$f(x_i, W, b) =  W x_i + b$$\n",
    "where\n",
    "$$x_i \\in R^D  (i = 1 \\dots N)$$\n",
    "$$y_i \\in { 1 \\dots K }$$\n",
    "\n",
    "The matrix **W** (of size [KxD]), and the vector **b** (of size [Kx1]) are the parameters of the function.The parameters in **W** are often called the weights, and **b** is called the bias vector because it influences the output scores, but without interacting with the actual data \\\\(x_i\\\\). However, you will often hear people use the terms weights and parameters interchangeably.\n",
    "\n",
    "For example, in CIFAR-10 we have a training set of N = 50,000 images, each with D = 32 x 32 x 3 = 3072 pixels, and K = 10. Some notes here:\n",
    "\n",
    "* \\\\(W x_i\\\\) is effectively evaluating 10 separate classifiers in parallel (one for each class), where each classifier is a row of W\n",
    "* The input data \\\\((x_i, y_i)\\\\) are given and fixed, we have control over the setting of **W** and **b**. Intuitively we wish that the correct class has a score that is higher than the scores of incorrect classes\n",
    "* Once the learning is complete, we can discard the entire training set and only keep the learned parameters\n",
    "* Classifying the test image involves a single matrix multiplication and addition, which is significantly faster than comparing a test image to all training images\n",
    "\n",
    "**Bias trick**(homogeneous equation)\n",
    "\n",
    "The new linear score function \\\\(f(x_i, W, b) =  W x_i + b\\\\) can be simplified to a single matrix multiply.\n",
    "\n",
    "$$f(x_i, W) =  W x_i$$\n",
    "because\n",
    "$$\n",
    "\\left[\\begin{array}{lcr}W & b \\end{array}\\right]\n",
    "\\left[\\begin{array}{lcr}x_i \\\\ 1 \\end{array}\\right]\n",
    "= W x_i + b\n",
    "$$\n",
    "\n",
    "With our CIFAR-10 example, \\\\(x_i\\\\) is now [3073 x 1] instead of [3072 x 1] , and **W** is now [10 x 3073] instead of [10 x 3072]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "We do have control over these weights and we want to set them so that the predicted class scores are consistent with the ground truth labels in the training data.\n",
    "\n",
    "We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the **cost** function or the **objective**). Intuitively, the loss will be high if we’re doing a poor job of classifying the training data, and it will be low if we’re doing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass Support Vector Machine loss\n",
    "The SVM loss is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin \\\\(\\Delta\\\\). The Multiclass SVM loss for the i-th example is then formalized as follows:\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n",
    "where the score for the j-th class is the j-th element (**s** short for scores)\n",
    "$$s_j = f(x_i, W)_j$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i(x, y, W):\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    correct_class_score = scores[y]\n",
    "    D = W.shape[0]\n",
    "    loss_i = 0.0\n",
    "    for j in range(D):\n",
    "        if j == y:\n",
    "            continue\n",
    "        loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - scores[y] + delta)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IMG = 33\n",
    "N_CLASS = 10\n",
    "\n",
    "W = np.random.rand(N_CLASS, D_IMG)\n",
    "x = np.random.randint(0, 255, D_IMG)\n",
    "y = 4 # 0 <= y < NUM_CLASS\n",
    "\n",
    "L_i(x, y, W), L_i_vectorized(x, y, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def L(X, Y, W):\n",
    "    \"\"\"\n",
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - Y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "    \n",
    "    \"\"\"\n",
    "    # evaluate loss over all examples in X without using any for loops\n",
    "    # left as exercise to reader in the assignment\n",
    "  \n",
    "    delta = 1.0\n",
    "    scores = W.dot(X)\n",
    "    # convert Y to one-hot matrix: 10x5,0000\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "Softmax classifier is the generalization of the binary Logistic Regression classifier to multiple classes. It gives a slightly more intuitive output and has a probalistic interpretation which normalizes class probalilites.\n",
    "\n",
    "**softmax function** $$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$$\n",
    "here \\\\(z_j\\\\) is the j-th element. It takes a vector **z** of arbitrary real-valued scores and squashes it to a vector of values between zero and one that sum to one.\n",
    "\n",
    "**cross-entropy** between a \"true\" distribution **p** and an estimated distribution **q** is defined as following in information theory.\n",
    "\n",
    "$$H(p,q) = - \\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "The cross-entropy can be written in terms of entropy and the Kullback-Leibler divergence as following.\n",
    "\n",
    "$$H(p,q) = H(p) + D_{KL}(p||q)$$\n",
    "\n",
    "**cross-entropy loss**\n",
    "$$L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}$$\n",
    "\n",
    "* minimizing the KL divergence between the two distributions (a measure of distance)\n",
    "* wants the predicted distribution to have all of its mass on the correct answer\n",
    "\n",
    "**Probabilistic interpretation** Looking at the expression, we see that\n",
    "$$P(y_i \\mid x_i; W) = \\frac{e^{f_{y_i}}}{\\sum_j e^{f_j} }$$\n",
    "\n",
    "we are therefore minimizing the negative log likelihood of the correct class, which can be interpreted as performing Maximum Likelihood Estimation (MLE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric stability\n",
    "When you're writing code for computing the Softmax function in practice, the interdediate terms may be very large due to the exponentials, such as \\\\(e^{f_{y_i}}\\\\) and \\\\(\\sum_j e^{f_j}\\\\). So it is important to use a normalization trick below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f = np.array([123, 456, 789])\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "\n",
    "f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "f -= np.max(f)\n",
    "p = np.exp(f) / np.sum(np.exp(f))\n",
    "\n",
    "f,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possibly confusing naming conventions\n",
    "To be precise, the SVM classifier uses the **hinge loss**, or also sometimes called the **max-margin loss**. The Softmax classifier uses the **cross-entropy loss**. The Softmax classifier gets its name from the **softmax function**, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. In particular, note that technically it doesn't make sense to talk about the \"softmax loss\", since softmax is just the squashing function, but it is a relatively commonly used shorthand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "We have a dataset and a set of parameters W that correctly classify every example.(\\\\(L_i = 0\\\\) for all i). One easy way to see this is that if one parameters W correctly classify all examples, then any multiple of these parameters \\\\(\\lambda W\\\\) where \\\\(\\lambda > 1\\\\) will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences.\n",
    "\n",
    "In other words, we wish to encode some preference for a certain set of weights **W** over others to remove this ambiguity. We can do so by extending the loss function with a regularization penalty \\\\(R(W)\\\\). The most common regularization penalty is the L2 norm that discourages large weights through an elementwise quadratic penalty over all parameters:\n",
    "\n",
    "$$R(W) = \\sum_k\\sum_l W_{k,l}^2$$\n",
    "\n",
    "That is, the full Multiclass SVM loss becomes:\n",
    "\n",
    "$$L =  \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss}$$\n",
    "\n",
    "The L2 penalty prefers smaller and more diffuse weight vectors, this effect can improve the generalization performance of the classifiers on test images and lead to less overfitting.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
