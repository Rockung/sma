{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Initialization\n",
    "Lets look at some activation statistics. E.g. 10-layer net with 500 neurons on each layer, using tanh non-linearities, and initializing as normal distribution.\n",
    "\n",
    "All activations become zero!\n",
    "\n",
    "Q: think about the backward pass. What do the gradients look like?\n",
    "\n",
    "Hint: think about backward pass for a W*X gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.randn(1000, 500)\n",
    "hidden_layer_sizes = [500] * 10\n",
    "nonlinearities = ['tanh'] * len(hidden_layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act = {'relu': lambda x: np.maximum(0, x), 'tanh': lambda x: np.tanh(x)}\n",
    "Hs = {}\n",
    "for i in range(len(hidden_layer_sizes)):\n",
    "    X = D if i == 0 else Hs[i-1]\n",
    "    fan_in = X.shape[1]\n",
    "    fan_out = hidden_layer_sizes[i]\n",
    "    \n",
    "    # Almost all neurons completely saturated, either -1 and 1. Grandients will be all zero.\n",
    "    # W = np.random.randn(fan_in, fan_out) * 1.0\n",
    "    \n",
    "    # W = np.random.randn(fan_in, fan_out) * 0.01\n",
    "    \n",
    "    # Xavier initialization [Glorot et al., 2010]\n",
    "    W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)\n",
    "    \n",
    "    H = np.dot(X, W)\n",
    "    H = act[nonlinearities[i]](H)\n",
    "    Hs[i] = H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('input layer had mean %f and std %f' % (np.mean(D), np.std(D)))\n",
    "layer_means = [np.mean(H) for i,H in Hs.items()]\n",
    "layer_stds = [np.std(H) for i,H in Hs.items()]\n",
    "for i,H in Hs.items():\n",
    "    print('hidden layer %d had mean %f and std %f' % (i+1, layer_means[i], layer_stds[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(Hs.keys(), layer_means, 'ob-')\n",
    "plt.title('layer mean')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(Hs.keys(), layer_stds, 'or-')\n",
    "plt.title('layer std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i,H in Hs.items():\n",
    "    plt.subplot(1, len(Hs), i+1)\n",
    "    plt.hist(H.ravel(), 30, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "To learn the plot functions\n",
    "* [matplotlib.pyplot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.html#module-matplotlib.pyplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
