{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Example\n",
    "Use backpropagation to compute the gradients for x and w of the function.\n",
    "$$f(w,x) = \\frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function \\\\(\\sigma(x)\\\\)\n",
    "The following are the sigmoid function and its derivative\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}} \\\\\\\\\n",
    "\\rightarrow \\hspace{0.2in} \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right) \n",
    "= \\left( 1 - \\sigma(x) \\right) \\sigma(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradients for \\\\(f(w,x)\\\\)\n",
    "Let \\\\(dot(w,x) = w_{0} x_{0} + w_{1} x_{1} + w_{2}\\\\), So \\\\(f(w,x) = \\sigma(dot(w,x))\\\\). We can now compute the gradients using chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = [2, -3, -3]\n",
    "x = [-1, -2]\n",
    "\n",
    "dot = w[0]*x[0] + w[1]*x[1] + w[2]\n",
    "sigma = 1.0 / (1 + np.exp(-dot))\n",
    "\n",
    "dsigma = (1 - sigma) * sigma\n",
    "dfdx = [w[0] * dsigma, w[1] * dsigma]\n",
    "dfdw = [x[0] * dsigma, x[1] * dsigma, 1.0 * dsigma]\n",
    "\n",
    "dfdx, dfdw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Staged Computation\n",
    "\n",
    "Suppose that we have a function of the form:\n",
    "\n",
    "$$f(x,y) = \\frac{x + \\sigma(y)}{\\sigma(x) + (x+y)^2}$$\n",
    "\n",
    "To be clear, this function is completely useless and it's not clear why you would ever want to compute its gradient, except for the fact that it is a good example of backpropagation in practice.\n",
    "\n",
    "It is very important to stress that if you were to launch into performing the differentiation with respect to either x or y, you would end up with very large and complex expressions. However, it turns out that doing so is completely unnecessary because we don’t need to have an explicit function written down that evaluates the gradient. We only have to know how to compute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = 3\n",
    "y = -4\n",
    "\n",
    "# **forward pass**\n",
    "\n",
    "# We have structured the code in such way that it contains multiple intermediate\n",
    "# variables, each of which are only simple expressions for which we already know\n",
    "# the local gradients. By the end of the expression we have computed the forward \n",
    "# pass. \n",
    "sigy = 1.0 / (1 +  np.exp(-y)) # sigmoid in numerator\n",
    "num = x + sigy                 # numerator\n",
    "sigx = 1.0 / (1 + np.exp(-x))  # signoid in denominator\n",
    "xpy = x + y                    # plus\n",
    "xpysqr = xpy**2\n",
    "den = sigx + xpysqr            # denominator\n",
    "invden = 1.0 / den             # inversion\n",
    "f = num * invden\n",
    "\n",
    "# **backward pass**\n",
    "\n",
    "# We’ll go backwards and for every variable along the way in the forward pass.\n",
    "# we will have the same variable, but one that begins with a **d**, which will \n",
    "# hold the gradient of the output of the circuit with respect to that variable.\n",
    "\n",
    "# Additionally, note that every single piece in our backprop will involve computing\n",
    "# the local gradient of that expression, and chaining it with the gradient on that\n",
    "# expression with a multiplication.\n",
    "\n",
    "dnum = invden                        # f = num * invden\n",
    "dinvden = num\n",
    "dden = (-1.0 / (den**2)) * dinvden   # invden = 1.0 / den \n",
    "dsigx = (1) * dden                   # den = sigx + xpysqr\n",
    "dxpysqr = (1) * dden                 \n",
    "dxpy = (2 * xpy) * dxpysqr           # xpysqr = xpy**2\n",
    "dx = (1) * dxpy\n",
    "dy = (1) * dxpy\n",
    "dx += ((1-sigx) * sigx) *dsigx       # sigx = 1.0 / (1 + np.exp(-x))\n",
    "dx += (1) * dnum\n",
    "dsigy = (1) * dnum\n",
    "dy += ((1 - sigy) * sigy) * dsigy    #sigy = 1.0 / (1 +  np.exp(-y))\n",
    "\n",
    "dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients add up at forks\n",
    "\n",
    "The forward expression involves the variables x,y multiple times, so when we perform backpropagation we must be careful to use **+=** instead of **=** to accumulate the gradient on these variables (otherwise we would overwrite it). This follows the multivariable chain rule in Calculus, which states that if a variable branches out to different parts of the circuit, then the gradients that flow back to it will add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patterns in backward flow\n",
    "\n",
    "It is interesting to note that in many cases the backward-flowing gradient can be interpreted on an intuitive level. For example, the three most commonly used gates in neural networks (add,mul,max), all have very simple interpretations in terms of how they act during backpropagation.\n",
    "\n",
    "* add gate: always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n",
    "* multiply gate: Its local gradients are the input values (except switched), and this is multiplied by the gradient on its output during the chain rule. It will assign a relatively huge gradient to the small input and a tiny gradient to the large input.\n",
    "* max gate: routes the gradient, distributes the gradient (unchanged) to exactly one of its inputs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients for vectorized operations\n",
    "\n",
    "The above sections were concerned with single variables, but all concepts extend in a straight-forward manner to matrix and vector operations. However, one must pay closer attention to dimensions and transpose operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# forward pass\n",
    "W = np.random.randn(5, 10)\n",
    "X = np.random.randn(10, 3)\n",
    "D = W.dot(X)\n",
    "\n",
    "# now suppose we had the gradient on D from above in the circuit\n",
    "dD = np.random.randn(*D.shape) # same shape as D\n",
    "dW = dD.dot(X.T)               # .T gives the transpose of the matrix\n",
    "dX = W.T.dot(dD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
