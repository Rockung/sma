{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "The linear score function and its SVM loss function we developed formulated as:\n",
    "\n",
    "$$f(x_i, W) =  W x_i$$\n",
    "$$L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + 1) \\right] + \\alpha R(W)$$\n",
    "$$R(W) = \\sum_k\\sum_l W_{k,l}^2$$\n",
    "\n",
    "A setting of the parameters \\\\(W\\\\) that produced predictions for examples \\\\(x_i\\\\) consistent with their ground truth labels \\\\(y_i\\\\) would also have a very low loss \\\\(L\\\\). We are now going to introduce the third and last key component: **optimization**. Optimization is the process of finding the set of parameters \\\\(W\\\\) that minimize the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random search\n",
    "The first (very bad) idea that may come to mind is to simply try out many different random weights and keep track of what works best. This procedure might look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n",
    "# assume Y_train are the labels (e.g. 1D array of 50,000)\n",
    "# assume the function L evaluates the loss function\n",
    "\n",
    "bestloss = float(\"inf\")\n",
    "for num in range(1000):\n",
    "    W = np.random.randn(10, 3073)\n",
    "    loss = L(X_train, Y_train, W)\n",
    "    if loss < bestloss:\n",
    "        bestloss = loss\n",
    "        bestW = W\n",
    "    print('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume X_test is [3073 x 10000], Y_test [10000 x 1]\n",
    "\n",
    "scores = Wbest.dot(X_test)\n",
    "Y_predict = np.argmax(scores, axis = 0)\n",
    "np.mean(Y_predict == Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Local Search\n",
    "\n",
    "We will start out with a random \\\\(W\\\\), generate random perturbations \\\\(\\delta W\\\\) to it and if the loss at the perturbed \\\\(W + \\delta W\\\\) is lower, we will perform an update. The code for this procedure is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(10, 3073) * 0.001\n",
    "bestLoss = float(\"inf\")\n",
    "for i in range(1000):\n",
    "    step_size = 0.0001\n",
    "    Wtry = W + np.random.randn(10, 3073) * step_size\n",
    "    loss = L(X_train, Y_train, Wtry)\n",
    "    if loss < bestLoss:\n",
    "        W = Wtry\n",
    "        bestLoss = loss\n",
    "    print('iter %d loss is %f' % (i, bestLoss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "We can compute the best direction along which we should change our weight vector that is mathematically guaranteed to be the direction of the steepest descend in the weight-space. This direction will be related to the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slope and Gradient\n",
    "In one-dimensional functions, the slope is the instantaneous rate of change of the function at any point you might be interested in.\n",
    "\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "The gradient is just a vector of slopes (more commonly referred to as **derivatives**) for each dimension in the input space. When the functions of interest take a vector of numbers instead of a single number, we call the derivatives **partial derivatives**, and the gradient is simply the vector of partial derivatives in each dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Gradient\n",
    "\n",
    "Following the gradient formula we gave above, the code below iterates over all dimensions one by one, makes a small change **h** along that dimension and calculates the partial derivative of the loss function along that dimension by seeing how much the function changed. The variable **grad** holds the full gradient in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x):\n",
    "    fx = f(x)\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h\n",
    "        fxh = f(x)\n",
    "        x[ix] = old_value\n",
    "        \n",
    "        grad[ix] = (fxh - fx) / h\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytic Gradient\n",
    "\n",
    "Lets use the example of the SVM loss function for a single datapoint:\n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]$$\n",
    "\n",
    "Taking the gradient with respect to \\\\(w_{y_i}\\\\)\n",
    "\n",
    "$$\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) \\right) x_i$$\n",
    "\n",
    "Notice that this is the gradient only with respect to the row of \\\\(W\\\\) that corresponds to the correct class. For the other rows where \\\\(j \\neq y_i\\\\) the gradient is:\n",
    "\n",
    "$$\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta > 0) x_i$$\n",
    "\n",
    "The gradient for regularization is:\n",
    "$$\\nabla_W R(W) = \\lambda W$$\n",
    "\n",
    "So, the whole formula of gradient for the SVM loss function is:\n",
    "\n",
    "$$\\nabla_W L(W) = \\frac{1}{N} \\sum_{i} \\nabla_{w_{j}} L_i + \\lambda W$$\n",
    "\n",
    "Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Gradient Descent\n",
    "\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_func, data, weights)\n",
    "    weights += (-step_size) * weights_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256)\n",
    "    weights_grad = evaluate_gradient(loss_func, data_batch, weights)\n",
    "    weights += (-step_size) * weights_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
